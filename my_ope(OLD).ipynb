{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, linewidth=150, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_true, y_pred):\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    FSCORE = np.divide((2*PPV*TPR), (PPV+TPR))\n",
    "    \n",
    "    return PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_particle = 'JetHTs'\n",
    "\n",
    "X_train = np.load(\"matrices/\" + name_of_particle +\"_train.npy\",)\n",
    "y_train = np.load(\"matrices/\" + name_of_particle +\"_y_train.npy\",)\n",
    "X_val = np.load(\"matrices/\" + name_of_particle +\"_val.npy\",)\n",
    "y_val = np.load(\"matrices/\" + name_of_particle +\"_y_val.npy\",)\n",
    "X_test = np.load(\"matrices/\" + name_of_particle +\"_test.npy\",)\n",
    "y_test = np.load(\"matrices/\" + name_of_particle +\"_y_test.npy\",)\n",
    "X_train = X_train[:, :-3]\n",
    "X_val = X_val[:, :-3]\n",
    "X_test = X_test[:, :-3]\n",
    "_, V = X_train.shape\n",
    "K = 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos = X_train[np.where(y_train==0)]\n",
    "X_train_neg = X_train[np.where(y_train==1)]\n",
    "y_train_pos = y_train[np.where(y_train==0)]\n",
    "y_train_neg = y_train[np.where(y_train==1)]\n",
    "\n",
    "# X_val_pos = X_val[np.where(y_val==0)]\n",
    "# X_val_neg = X_val[np.where(y_val==1)]\n",
    "# y_val_pos = y_val[np.where(y_val==0)]\n",
    "# y_val_neg = y_val[np.where(y_val==1)]\n",
    "\n",
    "# X_test_pos = X_test[np.where(y_test==0)]\n",
    "# X_test_neg = X_test[np.where(y_test==1)]\n",
    "# y_test_pos = y_test[np.where(y_test==0)]\n",
    "# y_test_neg = y_test[np.where(y_test==1)]\n",
    "\n",
    "X_train_pos.shape[0] + X_train_neg.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos = tf.convert_to_tensor(X_train_pos)\n",
    "y_train_pos = tf.convert_to_tensor(y_train_pos)\n",
    "X_train_neg = tf.convert_to_tensor(X_train_neg)\n",
    "y_train_neg = tf.convert_to_tensor(y_train_neg)\n",
    "\n",
    "X_val = tf.convert_to_tensor(X_val)\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# X_val_pos = tf.convert_to_tensor(X_val_pos)\n",
    "# y_val_pos = tf.convert_to_tensor(y_val_pos)\n",
    "# X_val_neg = tf.convert_to_tensor(X_val_neg)\n",
    "# y_val_neg = tf.convert_to_tensor(y_val_neg)\n",
    "\n",
    "\n",
    "# X_test_pos = tf.convert_to_tensor(X_test_pos[1:10, :])\n",
    "# y_test_pos = tf.convert_to_tensor(y_test_pos[1:10])\n",
    "# X_test_neg = tf.convert_to_tensor(X_test_neg[1:10, :])\n",
    "# y_test_neg = tf.convert_to_tensor(y_test_neg[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "\n",
    "# test_ds_pos = tf.data.Dataset.from_tensor_slices((X_test_pos, y_test_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# test_ds_neg = tf.data.Dataset.from_tensor_slices((X_test_neg, y_test_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# test_ds = (test_ds_pos, test_ds_neg)\n",
    "    \n",
    "# train_ds_pos = tf.data.Dataset.from_tensor_slices((X_train_pos, y_train_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# train_ds_neg = tf.data.Dataset.from_tensor_slices((X_train_neg, y_train_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# train_ds = (train_ds_pos, train_ds_neg)\n",
    "\n",
    "# val_ds_pos = tf.data.Dataset.from_tensor_slices((X_val_pos, y_val_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# val_ds_neg = tf.data.Dataset.from_tensor_slices((X_val_neg, y_val_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# val_ds = (val_ds_pos, val_ds_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the custom loss function for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bf(x_pos, x_neg):    \n",
    "    \n",
    "    p_pos = nn(x_pos)\n",
    "    p_neg = nn(x_neg)\n",
    "    \n",
    "    N_pos, V = x_pos.shape\n",
    "    N_neg, _ = x_neg.shape\n",
    "    N_pseudo = N_pos\n",
    "    \n",
    "    supp_min = tf.reduce_min(x_pos, keepdims=True, axis=0)\n",
    "    supp_max = tf.reduce_max(x_pos, keepdims=True, axis=0)\n",
    "    \n",
    "    x_pseudo = tf.random_uniform(shape=[N_pseudo, V],\n",
    "                                 minval=supp_min-1, maxval=supp_max+1,\n",
    "                                 dtype='float64',)\n",
    "    \n",
    "    p_psedo = model_occ(x_pseudo)\n",
    "    print(\"p_psedo\", p_psedo)\n",
    "    \n",
    "    N_pos = x_pos.shape[0]\n",
    "    N_neg = x_neg.shape[0]\n",
    "    \n",
    "    loss_pos = N_pos / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(-p_pos))\n",
    "    loss_neg = N_neg / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(p_neg))\n",
    "    loss_pseudo = 0.01 * tf.reduce_mean(tf.nn.softplus(p_psedo))\n",
    "    \n",
    "    print(\"loss_bf:\", loss_pos + loss_neg)\n",
    "    \n",
    "    return loss_pos + loss_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bf(X_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing an optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.BinaryCrossentropy()  (*)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics to measure the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss') \n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x_pos, x_neg, labels):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        predictions = compute_gradients()\n",
    "        \n",
    "    gradients = tape.gradient(predicions, model_occ.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_occ.trainable_variables))\n",
    "#     train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(X, labels):\n",
    "    predictions = model_occ(X)\n",
    "    labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "training_losses_occ, training_accuracies_occ = [], []\n",
    "validations_losses_occ, validations_accuracies_occ = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    results = train_step(x_pos=X_train_pos, x_neg=X_train_neg, labels=y_train)\n",
    "    test_step(X_val, y_val)\n",
    "    \n",
    "#     validations_losses_occ.append(test_loss.result().numpy())\n",
    "#     validations_accuracies_occ.append(test_accuracy.result().numpy())\n",
    "    \n",
    "#     training_losses_occ.append(train_loss.result().numpy())\n",
    "#     training_accuracies_occ.append(train_accuracy.result().numpy())\n",
    "\n",
    "#     validations_losses_occ.append(test_loss.result().numpy())\n",
    "#     validations_accuracies_occ.append(test_accuracy.result().numpy())\n",
    "    \n",
    "    template = 'Epoch {}, Validation Loss: {:.3f}, Validation Accuracy:{:.3f},'\n",
    "    \n",
    "    print (template.format(epoch+1,\n",
    "                           test_loss.result().numpy(),\n",
    "                           test_accuracy.result().numpy()*100),)\n",
    "    \n",
    "model_occ.save_weights(\"NN-ckecks/OCC_Bope\"+ name_of_particle +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_occ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_occ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_occ = get_model()\n",
    "new_model_occ.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds:\n",
    "    if cntr == 0:\n",
    "        new_model_occ.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "# new_model_occ.load_weights('NN-ckecks/ThreeLayerNN_model'+ name_of_particle+'.h5')\n",
    "test_predictions = new_model_occ.predict(X_test)\n",
    "probabilities = tf.nn.sigmoid(test_predictions)\n",
    "labels_pred_occ = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "labels_true_occ = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true_occ.append(k)\n",
    "\n",
    "f1_score_occ = precision_recall_fscore_support(labels_true_occ, labels_pred_occ, average='weighted') # Does not take into account labels imbalanced\n",
    "print(\"precision:\", \"%.2f\" %f1_score_occ[0], \"recall:\", \"%.2f\" % f1_score_occ[1], \"fscore:\", \"%.2f\" %f1_score_occ[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1, 0, 1, 1, 0]])\n",
    "y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((1*y_true).numpy(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([0, 1])\n",
    "n_features = 3\n",
    "\n",
    "n_neurons_in_h1 = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(X, dtype=tf.float32, name='X')\n",
    "y = tf.convert_to_tensor(Y, dtype=tf.float32, name='y')\n",
    "\n",
    "W1 = tf.random.normal(shape=[n_features, n_neurons_in_h1], \n",
    "                      mean=0.0, stddev=1/np.sqrt(n_features), name='W1')\n",
    "b1 = tf.random.normal(shape=[n_neurons_in_h1],\n",
    "                      mean=0.0, stddev=1/np.sqrt(n_features), name='b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_ = tf.Variable([[1, -1], [2, -2], [3, -3]], dtype='float32',name='W1')\n",
    "\n",
    "b1_ = tf.Variable([0.4, 0.9], name='b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W1_)\n",
    "print(b1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(x, W1_)+ b1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sigmoid(tf.matmul(x, W1_) + b1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "print(g.watch(x))\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
