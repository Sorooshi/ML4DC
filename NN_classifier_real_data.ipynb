{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import data_normalization\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_true, y_pred):\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    FSCORE = np.divide((2*PPV*TPR), (PPV+TPR))\n",
    "    \n",
    "    return PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Visulize Y with Quantatitve features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_particle = 'JetHTs'\n",
    "\n",
    "X_train = np.load(\"matrices/\" + name_of_particle +\"_train.npy\",)\n",
    "y_train = np.load(\"matrices/\" + name_of_particle +\"_y_train.npy\",)\n",
    "X_val = np.load(\"matrices/\" + name_of_particle +\"_val.npy\",)\n",
    "y_val = np.load(\"matrices/\" + name_of_particle +\"_y_val.npy\",)\n",
    "X_test = np.load(\"matrices/\" + name_of_particle +\"_test.npy\",)\n",
    "y_test = np.load(\"matrices/\" + name_of_particle +\"_y_test.npy\",)\n",
    "X_train = X_train[:, :-3]\n",
    "X_val = X_val[:, :-3]\n",
    "X_test = X_test[:, :-3]\n",
    "_, V = X_train.shape\n",
    "K = 2\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiliting Data and converting to tf.DataSet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).shuffle(1000)  \n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).shuffle(1000)  #shuffle(1000)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).shuffle(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N == len(y_train)+len(y_test)+ len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreeLayerNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNN(Model):\n",
    "    def __init__(self, name=None):\n",
    "        \n",
    "        super(ThreeLayerNN, self).__init__(name=name)\n",
    "        # super(name=name)\n",
    "\n",
    "        self.dense1 = Dense(32, activation='relu', input_shape=(V,),\n",
    "                            kernel_regularizer='l2', bias_regularizer='l2',  \n",
    "                            kernel_initializer='uniform',\n",
    "                            name='dense_1')\n",
    "        self.dropout1 = Dropout(0.25)\n",
    "        \n",
    "        self.dense2 = Dense(16, activation='relu', \n",
    "                            kernel_regularizer='l2', bias_regularizer='l2',\n",
    "                            name='dense_2')\n",
    "        self.dropout2 = Dropout(0.25)\n",
    "        \n",
    "        self.pred_layer = Dense(2, activation='sigmoid',\n",
    "                                kernel_initializer='uniform',\n",
    "                                name='predictions')  # output layer \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.pred_layer(x)\n",
    "    \n",
    "def get_model():\n",
    "    return ThreeLayerNN(name='3_layer_nn')\n",
    "            \n",
    "model_nn3 = get_model() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an optimizer and loss function for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()  \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics to measure the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train \n",
    "using eager execution with tf.GradientTape to record the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(X, labels):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        predictions = model_nn3(X)\n",
    "        print(\"predictions\", predictions)\n",
    "        labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "        loss = loss_object(labels, predictions)\n",
    "        print(\"loss:\", loss)\n",
    "    gradients = tape.gradient(loss, model_nn3.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_nn3.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(X, labels):\n",
    "    predictions = model_nn3(X)\n",
    "    labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model ThreeLayerMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1013 19:04:06.009195 140362191300416 base_layer.py:1814] Layer 3_layer_nn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "predictions Tensor(\"3_layer_nn/predictions/Sigmoid:0\", shape=(512, 2), dtype=float32)\n",
      "loss: Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_537 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "predictions Tensor(\"3_layer_nn/predictions/Sigmoid:0\", shape=(512, 2), dtype=float32)\n",
      "loss: Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Executing op __inference_train_step_760 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "predictions Tensor(\"3_layer_nn/predictions/Sigmoid:0\", shape=(102, 2), dtype=float32)\n",
      "loss: Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Executing op __inference_train_step_1003 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_test_step_1436 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_test_step_1535 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Epoch 1, Train Loss: 0.684, Train Accuracy:87.116,     Validation Loss: 0.683, Validation Accuracy:88.295,\n",
      "Epoch 2, Train Loss: 0.683, Train Accuracy:89.014,     Validation Loss: 0.682, Validation Accuracy:90.087,\n",
      "Epoch 3, Train Loss: 0.682, Train Accuracy:90.774,     Validation Loss: 0.682, Validation Accuracy:91.664,\n",
      "Epoch 4, Train Loss: 0.681, Train Accuracy:92.289,     Validation Loss: 0.681, Validation Accuracy:93.122,\n",
      "Epoch 5, Train Loss: 0.680, Train Accuracy:93.598,     Validation Loss: 0.680, Validation Accuracy:94.281,\n",
      "Epoch 6, Train Loss: 0.679, Train Accuracy:94.675,     Validation Loss: 0.679, Validation Accuracy:95.293,\n",
      "early_stop: False\n",
      "Epoch 7, Train Loss: 0.678, Train Accuracy:95.583,     Validation Loss: 0.677, Validation Accuracy:96.076,\n",
      "early_stop: False\n",
      "Epoch 8, Train Loss: 0.677, Train Accuracy:96.241,     Validation Loss: 0.676, Validation Accuracy:96.714,\n",
      "early_stop: False\n",
      "Epoch 9, Train Loss: 0.676, Train Accuracy:96.813,     Validation Loss: 0.675, Validation Accuracy:97.184,\n",
      "early_stop: False\n",
      "Epoch 10, Train Loss: 0.675, Train Accuracy:97.237,     Validation Loss: 0.674, Validation Accuracy:97.570,\n",
      "early_stop: False\n",
      "Epoch 11, Train Loss: 0.673, Train Accuracy:97.565,     Validation Loss: 0.673, Validation Accuracy:97.833,\n",
      "early_stop: False\n",
      "Epoch 12, Train Loss: 0.672, Train Accuracy:97.829,     Validation Loss: 0.672, Validation Accuracy:98.040,\n",
      "early_stop: False\n",
      "Epoch 13, Train Loss: 0.671, Train Accuracy:98.011,     Validation Loss: 0.670, Validation Accuracy:98.190,\n",
      "early_stop: False\n",
      "Epoch 14, Train Loss: 0.670, Train Accuracy:98.168,     Validation Loss: 0.669, Validation Accuracy:98.328,\n",
      "early_stop: False\n",
      "Epoch 15, Train Loss: 0.668, Train Accuracy:98.290,     Validation Loss: 0.668, Validation Accuracy:98.448,\n",
      "early_stop: False\n",
      "Epoch 16, Train Loss: 0.667, Train Accuracy:98.374,     Validation Loss: 0.667, Validation Accuracy:98.537,\n",
      "early_stop: False\n",
      "Epoch 17, Train Loss: 0.666, Train Accuracy:98.445,     Validation Loss: 0.665, Validation Accuracy:98.582,\n",
      "early_stop: False\n",
      "Epoch 18, Train Loss: 0.664, Train Accuracy:98.502,     Validation Loss: 0.664, Validation Accuracy:98.613,\n",
      "early_stop: False\n",
      "Epoch 19, Train Loss: 0.663, Train Accuracy:98.533,     Validation Loss: 0.663, Validation Accuracy:98.664,\n",
      "early_stop: False\n",
      "Epoch 20, Train Loss: 0.662, Train Accuracy:98.558,     Validation Loss: 0.661, Validation Accuracy:98.693,\n",
      "early_stop: False\n",
      "Epoch 21, Train Loss: 0.660, Train Accuracy:98.578,     Validation Loss: 0.660, Validation Accuracy:98.702,\n",
      "early_stop: False\n",
      "Epoch 22, Train Loss: 0.659, Train Accuracy:98.594,     Validation Loss: 0.658, Validation Accuracy:98.711,\n",
      "early_stop: False\n",
      "Epoch 23, Train Loss: 0.657, Train Accuracy:98.605,     Validation Loss: 0.657, Validation Accuracy:98.722,\n",
      "early_stop: False\n",
      "Epoch 24, Train Loss: 0.656, Train Accuracy:98.614,     Validation Loss: 0.655, Validation Accuracy:98.729,\n",
      "early_stop: False\n",
      "Epoch 25, Train Loss: 0.654, Train Accuracy:98.622,     Validation Loss: 0.654, Validation Accuracy:98.740,\n",
      "early_stop: False\n",
      "Epoch 26, Train Loss: 0.653, Train Accuracy:98.630,     Validation Loss: 0.652, Validation Accuracy:98.743,\n",
      "early_stop: False\n",
      "Epoch 27, Train Loss: 0.651, Train Accuracy:98.633,     Validation Loss: 0.651, Validation Accuracy:98.745,\n",
      "early_stop: False\n",
      "Epoch 28, Train Loss: 0.650, Train Accuracy:98.636,     Validation Loss: 0.649, Validation Accuracy:98.745,\n",
      "early_stop: False\n",
      "Epoch 29, Train Loss: 0.648, Train Accuracy:98.636,     Validation Loss: 0.648, Validation Accuracy:98.745,\n",
      "early_stop: False\n",
      "Epoch 30, Train Loss: 0.647, Train Accuracy:98.638,     Validation Loss: 0.646, Validation Accuracy:98.745,\n",
      "early_stop: False\n",
      "Epoch 31, Train Loss: 0.645, Train Accuracy:98.639,     Validation Loss: 0.645, Validation Accuracy:98.747,\n",
      "early_stop: False\n",
      "Epoch 32, Train Loss: 0.644, Train Accuracy:98.641,     Validation Loss: 0.643, Validation Accuracy:98.747,\n",
      "early_stop: False\n",
      "Epoch 33, Train Loss: 0.642, Train Accuracy:98.641,     Validation Loss: 0.641, Validation Accuracy:98.749,\n",
      "early_stop: False\n",
      "Epoch 34, Train Loss: 0.640, Train Accuracy:98.641,     Validation Loss: 0.640, Validation Accuracy:98.749,\n",
      "early_stop: False\n",
      "Epoch 35, Train Loss: 0.639, Train Accuracy:98.642,     Validation Loss: 0.638, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 36, Train Loss: 0.637, Train Accuracy:98.642,     Validation Loss: 0.636, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 37, Train Loss: 0.635, Train Accuracy:98.642,     Validation Loss: 0.634, Validation Accuracy:98.751,\n",
      "early_stop: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Train Loss: 0.633, Train Accuracy:98.642,     Validation Loss: 0.633, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 39, Train Loss: 0.632, Train Accuracy:98.643,     Validation Loss: 0.631, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 40, Train Loss: 0.630, Train Accuracy:98.643,     Validation Loss: 0.629, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 41, Train Loss: 0.628, Train Accuracy:98.643,     Validation Loss: 0.627, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 42, Train Loss: 0.626, Train Accuracy:98.643,     Validation Loss: 0.626, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 43, Train Loss: 0.624, Train Accuracy:98.643,     Validation Loss: 0.624, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 44, Train Loss: 0.623, Train Accuracy:98.643,     Validation Loss: 0.622, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 45, Train Loss: 0.621, Train Accuracy:98.643,     Validation Loss: 0.620, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 46, Train Loss: 0.619, Train Accuracy:98.643,     Validation Loss: 0.618, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 47, Train Loss: 0.617, Train Accuracy:98.643,     Validation Loss: 0.616, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 48, Train Loss: 0.615, Train Accuracy:98.643,     Validation Loss: 0.614, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 49, Train Loss: 0.613, Train Accuracy:98.643,     Validation Loss: 0.613, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 50, Train Loss: 0.611, Train Accuracy:98.643,     Validation Loss: 0.611, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 51, Train Loss: 0.609, Train Accuracy:98.643,     Validation Loss: 0.609, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 52, Train Loss: 0.607, Train Accuracy:98.643,     Validation Loss: 0.607, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 53, Train Loss: 0.605, Train Accuracy:98.643,     Validation Loss: 0.605, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 54, Train Loss: 0.603, Train Accuracy:98.643,     Validation Loss: 0.603, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 55, Train Loss: 0.601, Train Accuracy:98.643,     Validation Loss: 0.601, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 56, Train Loss: 0.599, Train Accuracy:98.643,     Validation Loss: 0.599, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 57, Train Loss: 0.597, Train Accuracy:98.643,     Validation Loss: 0.597, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 58, Train Loss: 0.595, Train Accuracy:98.643,     Validation Loss: 0.595, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 59, Train Loss: 0.593, Train Accuracy:98.643,     Validation Loss: 0.592, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 60, Train Loss: 0.591, Train Accuracy:98.643,     Validation Loss: 0.590, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 61, Train Loss: 0.589, Train Accuracy:98.643,     Validation Loss: 0.588, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 62, Train Loss: 0.587, Train Accuracy:98.643,     Validation Loss: 0.586, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 63, Train Loss: 0.585, Train Accuracy:98.643,     Validation Loss: 0.584, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 64, Train Loss: 0.582, Train Accuracy:98.643,     Validation Loss: 0.582, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 65, Train Loss: 0.580, Train Accuracy:98.643,     Validation Loss: 0.580, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 66, Train Loss: 0.578, Train Accuracy:98.643,     Validation Loss: 0.577, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 67, Train Loss: 0.576, Train Accuracy:98.643,     Validation Loss: 0.575, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 68, Train Loss: 0.574, Train Accuracy:98.643,     Validation Loss: 0.573, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 69, Train Loss: 0.571, Train Accuracy:98.643,     Validation Loss: 0.571, Validation Accuracy:98.751,\n",
      "early_stop: False\n",
      "Epoch 70, Train Loss: 0.569, Train Accuracy:98.643,     Validation Loss: 0.569, Validation Accuracy:98.751,\n",
      "early_stop: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-91817a2b03c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# X_tr := train iterator from traning data set (train_ds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtraining_losses_nn3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "early_stop_counter = 0 \n",
    "delta = 0.0001  # the differenece between two consecutive validation losses (accuracies) for early stop\n",
    "patience = 5 \n",
    "early_stop = False\n",
    "\n",
    "training_losses_nn3, training_accuracies_nn3 = [], []\n",
    "validations_losses_nn3, validations_accuracies_nn3 = [], []\n",
    "i = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for X_tr, y_tr in train_ds:  # X_tr := train iterator from traning data set (train_ds)\n",
    "        results = train_step(X_tr, y_tr)\n",
    "    training_losses_nn3.append(train_loss.result().numpy())\n",
    "    training_accuracies_nn3.append(train_accuracy.result().numpy())\n",
    "    \n",
    "    for X_val, y_val in val_ds:  \n",
    "        test_step(X_val, y_val)\n",
    "    validations_losses_nn3.append(test_loss.result().numpy())\n",
    "    validations_accuracies_nn3.append(test_accuracy.result().numpy())\n",
    "    \n",
    "    # checkpoint_path. i.e weight and layers and ect.\n",
    "    # For the case of Server Failures or etc\n",
    "    \n",
    "#     if epoch % 10 == 0: \n",
    "#         checkpoint_path = \"NN-ckecks/ThreeLayerNN6_model_\" + name_of_particle + str(epoch)\n",
    "#         model_nn3.save_weights(checkpoint_path, save_format='tf')\n",
    "    \n",
    "    training_losses_nn3.append(train_loss.result().numpy())\n",
    "    training_accuracies_nn3.append(train_accuracy.result().numpy())\n",
    "\n",
    "    validations_losses_nn3.append(test_loss.result().numpy())\n",
    "    validations_accuracies_nn3.append(test_accuracy.result().numpy())\n",
    "    \n",
    "    template = 'Epoch {}, Train Loss: {:.3f}, Train Accuracy:{:.3f}, \\\n",
    "    Validation Loss: {:.3f}, Validation Accuracy:{:.3f},'\n",
    "    \n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result().numpy(),\n",
    "                         train_accuracy.result().numpy()*100,\n",
    "                           \n",
    "                         test_loss.result().numpy(),\n",
    "                         test_accuracy.result().numpy()*100),)\n",
    "    \n",
    "    if epoch >= 5:\n",
    "        \n",
    "        history = validations_losses_nn3[-5:]\n",
    "        for i in range(len(history)):\n",
    "            if i < len(history)-1:\n",
    "                if np.abs(history[i+1]-history[i]) <= delta:\n",
    "                    early_stop_counter += 1\n",
    "        if early_stop_counter == patience-1:\n",
    "            early_stop = True\n",
    "\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        print(\"early_stop:\", early_stop)\n",
    "\n",
    "    # Reset metrics for the next epochs\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "#     if early_stop is True:\n",
    "#         break\n",
    "    \n",
    "model_nn3.save_weights(\"NN-ckecks/ThreeLayerNN_model\"+ name_of_particle +\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The summary of theTrained Model (ThreeLayerNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very good however, the data set is not a tricky one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complex Network \n",
    "\n",
    "### SixLayerNN classifier with more Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SixLayerNN(Model):\n",
    "    def __init__(self, name=None):\n",
    "        super(SixLayerNN, self).__init__(name=name)\n",
    "        self.dense1 = Dense(256, activation='relu', kernel_regularizer='l2', \n",
    "                            bias_regularizer='l2', input_shape=(V,), name='dense_1',\n",
    "                            kernel_initializer='normal'\n",
    "                           )\n",
    "        self.dropout1 = Dropout(0.25)\n",
    "        \n",
    "        self.dense2 = Dense(128, activation='relu', \n",
    "                            kernel_regularizer='l2', bias_regularizer='l2', name='dense_2'\n",
    "                           )\n",
    "        self.dropout2 = Dropout(0.25)\n",
    "        \n",
    "        self.dense3 = Dense(64, activation='relu', \n",
    "                            kernel_regularizer='l2', bias_regularizer='l2', name='dense_3'\n",
    "                           )\n",
    "        self.dropout3 = Dropout(0.25)\n",
    "        \n",
    "        self.dense4 = Dense(32, activation='relu', \n",
    "                            kernel_regularizer='l2', bias_regularizer='l2', name='dense_4'\n",
    "                           )\n",
    "        self.dropout4 = Dropout(0.25)\n",
    "        \n",
    "        self.dense5 = Dense(16, activation='relu', \n",
    "                            kernel_regularizer='l2', bias_regularizer='l2', name='dense_5'\n",
    "                           )\n",
    "        self.dropout5 = Dropout(0.25)\n",
    "        \n",
    "        self.pred_layer = Dense(2, activation='sigmoid',\n",
    "                               name='predictions')  # output layer \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dropout5(x)\n",
    "        return self.pred_layer(x)\n",
    "    \n",
    "def get_model6():\n",
    "    return SixLayerNN(name='6_layer_nn')\n",
    "            \n",
    "model_nn6 = get_model6() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an optimizer and loss function for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object6 = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer6 = tf.keras.optimizers.Adam(learning_rate=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics to measure the loss and accuracy of the model\n",
    "(I think it was possible to use the previously defined functions but I decided to defined them once more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss6 = tf.keras.metrics.Mean(name='train_loss6')\n",
    "train_accuracy6 = tf.keras.metrics.BinaryAccuracy(name='train_accuracy6')\n",
    "\n",
    "test_loss6 = tf.keras.metrics.Mean(name='test_loss6')\n",
    "test_accuracy6 = tf.keras.metrics.BinaryAccuracy(name='test_accuracy6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train \n",
    "using eager execution with tf.GradientTape to record the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step6(X, labels):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        predictions6 = model_nn6(X)\n",
    "        labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "        loss6 = loss_object6(labels, predictions6)\n",
    "    \n",
    "    gradients6 = tape.gradient(loss6, model_nn6.trainable_variables)\n",
    "    optimizer6.apply_gradients(zip(gradients6, model_nn6.trainable_variables))\n",
    "    \n",
    "    train_loss6(loss6)\n",
    "    train_accuracy6(labels, predictions6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step6(X, labels):\n",
    "    predictions6 = model_nn6(X)\n",
    "    labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "    t_loss6 = loss_object6(labels, predictions6)\n",
    "    \n",
    "    test_loss6(t_loss6)\n",
    "    test_accuracy6(labels, predictions6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model SixLayerMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "early_stop_counter = 0 \n",
    "delta = 0.0001  # the differenece between two consecutive validation losses (accuracies) for early stop\n",
    "patience = 5 \n",
    "early_stop = False\n",
    "\n",
    "\n",
    "training_losses_nn6, training_accuracies_nn6 = [], []\n",
    "validations_losses_nn6, validations_accuracies_nn6 = [], []\n",
    "i = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for X_tr, y_tr in train_ds:  # X_tr := train iterator from traning data set (train_ds)\n",
    "        results = train_step6(X_tr, y_tr)\n",
    "    training_losses_nn6.append(train_loss6.result().numpy())\n",
    "    training_accuracies_nn6.append(train_accuracy6.result().numpy())\n",
    "    \n",
    "    for X_val, y_val in val_ds:  \n",
    "        test_step6(X_val, y_val)\n",
    "    validations_losses_nn6.append(test_loss6.result().numpy())\n",
    "    validations_accuracies_nn6.append(test_accuracy6.result().numpy())\n",
    "    \n",
    "    # checkpoint_path. i.e weight and layers and ect.\n",
    "    # For the case of Server Failures or etc\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         checkpoint_path = \"NN-ckecks/SixLayerNN6_model_\" + name_of_particle + str(epoch)\n",
    "#         model_nn6.save_weights(checkpoint_path, save_format='tf')\n",
    "    \n",
    "    training_losses_nn6.append(train_loss6.result().numpy())\n",
    "    training_accuracies_nn6.append(train_accuracy6.result().numpy())\n",
    "\n",
    "    validations_losses_nn6.append(test_loss6.result().numpy())\n",
    "    validations_accuracies_nn6.append(test_accuracy6.result().numpy())\n",
    "    \n",
    "    template = 'Epoch {}, Train Loss: {:.3f}, Train Accuracy:{:.3f}, \\\n",
    "    Validation Loss: {:.3f}, Validation Accuracy:{:.3f},'\n",
    "    _nn3\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss6.result().numpy(),\n",
    "                         train_accuracy6.result().numpy()*100,\n",
    "                           \n",
    "                         test_loss6.result().numpy(),\n",
    "                         test_accuracy6.result().numpy()*100),)\n",
    "    \n",
    "    if epoch >= 5:\n",
    "        \n",
    "        history = validations_losses_nn6[-5:]\n",
    "        for i in range(len(history)):\n",
    "            if i < len(history)-1:\n",
    "                if np.abs(history[i+1]-history[i]) <= delta:\n",
    "                    early_stop_counter += 1\n",
    "        if early_stop_counter == patience-1:\n",
    "            early_stop = True\n",
    "\n",
    "        early_stop_counter = 0\n",
    "\n",
    "\n",
    "        print(\"early_stop:\", early_stop)\n",
    "\n",
    "    # Reset metrics for the next epochs\n",
    "    train_loss6.reset_states()\n",
    "    train_accuracy6.reset_states()\n",
    "    test_loss6.reset_states()\n",
    "    test_accuracy6.reset_states()\n",
    "    \n",
    "    \n",
    "#     if early_stop is True:\n",
    "#         break\n",
    "    \n",
    "model_nn6.save_weights(\"NN-ckecks/SixLayerNN_model\"+ name_of_particle +\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The summaru of theTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting the train and validation losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=[15, 10.5])\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(training_accuracies_nn3, 'b-.', validations_accuracies_nn3, 'g-.',\n",
    "         training_accuracies_nn6, 'm-+', validations_accuracies_nn6, 'y-+',)\n",
    "\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"classification accuracy\")\n",
    "plt.legend([\"3NN-training\", \"3NN-valid\", \"6NN-training\", \"6NN-valid\",])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(training_losses_nn3, 'b-.', validations_losses_nn3, 'g-.',\n",
    "         training_losses_nn6, 'm-+', validations_losses_nn6, 'y-+',)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"classification loss\")\n",
    "plt.legend([\"3NN-training\", \"3NN-valid\", \"6NN-training\", \"6NN-valid\",])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why is the training loss (much) higher than the testing (Validation) loss?\n",
    "\n",
    "A Keras model has two modes: training and testing(in above validation).\n",
    "Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing(validation) time.\n",
    "\n",
    "Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "\n",
    "I guess that in above case the presence of dropout and legularization especially prevent the accuracy from going to 1.0 during training, while it achieves it during evaluation (testing). \n",
    "\n",
    "###### NOTE: /\n",
    "If you think I am mistaken, I could plot the falsify points and check my claim. \n",
    "(It's a bit tricky but it is not impossible)\n",
    "\n",
    "### Advantages of more complex network\n",
    "\n",
    "At one can perceive adding more layers and Neuron is equivalent to increasing the speed of convergence which could be indeed useful in real-world applications.\n",
    "Moreover, I expect that with utilizing the more complex network the better results will obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the save model - NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_nn3 = get_model()\n",
    "new_model_nn3.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds:\n",
    "    if cntr == 0:\n",
    "        new_model_nn3.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "new_model_nn3.load_weights('NN-ckecks/ThreeLayerNN_model'+ name_of_particle+'.h5')\n",
    "test_predictions = new_model_nn3.predict(X_test)\n",
    "probabilities = tf.nn.sigmoid(test_predictions)\n",
    "labels_pred_nn3 = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "labels_true_nn3 = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true_nn3.append(k)\n",
    "\n",
    "f1_score_nn3 = precision_recall_fscore_support(labels_true_nn3, labels_pred_nn3, average='weighted') # Does not take into account labels imbalanced\n",
    "print(\"precision:\", \"%.2f\" %f1_score_nn3[0], \"recall:\", \"%.2f\" % f1_score_nn3[1], \"fscore:\", \"%.2f\" %f1_score_nn3[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_nn3 = precision_recall_fscore_support(labels_true_nn3, labels_pred_nn3,) # average='weighted') # Does not take into account labels imbalanced\n",
    "# print(\"precision:\", f1_score_nn3[0], \"recall:\", f1_score_nn3[1], \"fscore:\", f1_score_nn3[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the save model - NN6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_nn6 = get_model6()\n",
    "new_model_nn6.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds:\n",
    "    if cntr == 0:\n",
    "        new_model_nn6.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "new_model_nn6.load_weights('NN-ckecks/SixLayerNN_model'+ name_of_particle +'.h5')\n",
    "test_predictions_ = new_model_nn6.predict(X_test)\n",
    "probabilities_ = tf.nn.sigmoid(test_predictions_)\n",
    "labels_pred_nn6 = tf.argmax(probabilities_, axis=1)\n",
    "\n",
    "labels_true_nn6 = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true_nn6.append(k)\n",
    "\n",
    "f1_score_nn6 = precision_recall_fscore_support(labels_true_nn6, labels_pred_nn6, average='weighted') # Does not take into account labels imbalanced\n",
    "print(\"precision-6NN:\", \"%.2f\" % f1_score_nn6[0], \"recall-6NN:\", \"%.2f\" % f1_score_nn6[1], \"fscore-6NN:\", \"%.2f\" % f1_score_nn6[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_nn6 = precision_recall_fscore_support(labels_true_nn6, labels_pred_nn6,) # average='weighted') # Does not take into account labels imbalanced\n",
    "# print(\"precision:\", f1_score_nn6[0], \"recall:\", f1_score_nn6[1], \"fscore:\", f1_score_nn6[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPV3, TPR3, FSCORE3, FNR3, FPR3, TNR3 = perf_measure(y_true=labels_true_nn6, y_pred=labels_pred_nn3)\n",
    "\n",
    "PPV6, TPR6, FSCORE6, FNR6, FPR6, TNR6 = perf_measure(y_true=labels_true_nn6, y_pred=labels_pred_nn6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPV3, TPR3, FSCORE3, FNR3, FPR3, TNR3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(array([0.97, 0.01]),\n",
    " array([0., 1.]),\n",
    " array([0.  , 0.03]),\n",
    " array([1., 0.]),\n",
    " array([0., 1.]),\n",
    " array([1., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPV6, TPR6, FSCORE6, FNR6, FPR6, TNR6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
