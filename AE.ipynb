{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1010 13:03:56.595734 139930587498304 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W1010 13:03:56.613404 139930587498304 __init__.py:335] Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_true, y_pred):\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    FSCORE = np.divide((2*PPV*TPR), (PPV+TPR))\n",
    "    \n",
    "    return PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_particle = 'Egammas'\n",
    "\n",
    "X_train = np.load(\"matrices/\" + name_of_particle +\"_train.npy\",)\n",
    "y_train = np.load(\"matrices/\" + name_of_particle +\"_y_train.npy\",)\n",
    "X_val = np.load(\"matrices/\" + name_of_particle +\"_val.npy\",)\n",
    "y_val = np.load(\"matrices/\" + name_of_particle +\"_y_val.npy\",)\n",
    "X_test = np.load(\"matrices/\" + name_of_particle +\"_test.npy\",)\n",
    "y_test = np.load(\"matrices/\" + name_of_particle +\"_y_test.npy\",)\n",
    "X_train = X_train[:, :-3]\n",
    "X_val = X_val[:, :-3]\n",
    "X_test = X_test[:, :-3]\n",
    "_, V = X_train.shape\n",
    "K = 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pos = X_train[np.where(y_train==0)]\n",
    "X_train_neg = X_train[np.where(y_train==1)]\n",
    "y_train_pos = y_train[np.where(y_train==0)]\n",
    "y_train_neg = y_train[np.where(y_train==1)]\n",
    "\n",
    "X_val_pos = X_val[np.where(y_val==0)]\n",
    "X_val_neg = X_val[np.where(y_val==1)]\n",
    "y_val_pos = y_val[np.where(y_val==0)]\n",
    "y_val_neg = y_val[np.where(y_val==1)]\n",
    "\n",
    "X_test_pos = X_test[np.where(y_test==0)]\n",
    "X_test_neg = X_test[np.where(y_test==1)]\n",
    "y_test_pos = y_test[np.where(y_test==0)]\n",
    "y_test_neg = y_test[np.where(y_test==1)]\n",
    "\n",
    "X_train_pos.shape[0] + X_train_neg.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "test_ds_pos = tf.data.Dataset.from_tensor_slices((X_test_pos, y_test_pos)).batch(batch_size) #.shuffle(1000)\n",
    "test_ds_neg = tf.data.Dataset.from_tensor_slices((X_test_neg, y_test_neg)).batch(batch_size)#.shuffle(1000)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "    \n",
    "train_ds_pos = tf.data.Dataset.from_tensor_slices((X_train_pos, y_train_pos)).batch(batch_size)#.shuffle(1000)\n",
    "train_ds_neg = tf.data.Dataset.from_tensor_slices((X_train_neg, y_train_neg)).batch(batch_size)#.shuffle(1000)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)#.shuffle(1000)\n",
    "\n",
    "val_ds_pos = tf.data.Dataset.from_tensor_slices((X_val_pos, y_val_pos)).batch(batch_size)#.shuffle(1000)\n",
    "val_ds_neg = tf.data.Dataset.from_tensor_slices((X_val_neg, y_val_neg)).batch(batch_size)#.shuffle(1000)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)#.shuffle(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, original_dim, intermediate_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.h1 = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu, input_shape=(original_dim,))\n",
    "        self.output_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
    "    \n",
    "    def call(self, x):\n",
    "        h1 = self.h1(x)\n",
    "        return self.output_layer(h1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.h1 = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu, input_shape=(intermediate_dim,))\n",
    "        self.output_layer = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu)\n",
    "    \n",
    "    def call(self, z):\n",
    "        h1 = self.h1(z)\n",
    "        return self.output_layer(h1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(original_dim=original_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, original):\n",
    "    reconstruction_error = tf.reduce_mean(tf.square(model(original), original))\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, model, opt, original):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 5\n",
      "epoch: 10\n",
      "epoch: 15\n",
      "epoch: 20\n",
      "epoch: 25\n",
      "epoch: 30\n",
      "epoch: 35\n",
      "epoch: 40\n",
      "epoch: 45\n",
      "epoch: 50\n",
      "epoch: 55\n",
      "epoch: 60\n",
      "epoch: 65\n",
      "epoch: 70\n",
      "epoch: 75\n",
      "epoch: 80\n",
      "epoch: 85\n",
      "epoch: 90\n",
      "epoch: 95\n",
      "epoch: 100\n",
      "epoch: 105\n",
      "epoch: 110\n",
      "epoch: 115\n",
      "epoch: 120\n",
      "epoch: 125\n",
      "epoch: 130\n",
      "epoch: 135\n",
      "epoch: 140\n",
      "epoch: 145\n",
      "epoch: 150\n",
      "epoch: 155\n",
      "epoch: 160\n",
      "epoch: 165\n",
      "epoch: 170\n",
      "epoch: 175\n",
      "epoch: 180\n",
      "epoch: 185\n",
      "epoch: 190\n",
      "epoch: 195\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "autoencoder = AutoEncoder(intermediate_dim=100, original_dim=V)\n",
    "opt = tf.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for X_tr, y_tr in train_ds:\n",
    "        train(loss, autoencoder, opt, X_tr)\n",
    "        loss_values = loss(autoencoder, X_tr)\n",
    "        original = tf.reshape(X_tr, (X_tr.shape[0], V, 1))\n",
    "        reconstructed = tf.reshape(autoencoder(tf.constant(X_tr)), (X_tr.shape[0], V, 1))\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(\"epoch:\", epoch)\n",
    "\n",
    "autoencoder.save_weights(\"NN-ckecks/AutoEncoder\"+ name_of_particle +\".h5\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  65932     \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  31411     \n",
      "=================================================================\n",
      "Total params: 97,343\n",
      "Trainable params: 97,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 13:09:21.601952 139930587498304 deprecation.py:323] From /home/Soroosh/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "new_autoencoder = AutoEncoder(intermediate_dim=100, original_dim=V)\n",
    "new_autoencoder.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds_pos:\n",
    "    if cntr == 0:\n",
    "        new_autoencoder.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "new_autoencoder.load_weights(\"NN-ckecks/AutoEncoder\"+ name_of_particle +\".h5\")\n",
    "test_predictions = new_autoencoder.predict(X_test)\n",
    "probabilities = tf.nn.sigmoid(test_predictions)\n",
    "labels_pred = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "labels_true = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true.append(int(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
    "df_error = pd.DataFrame({'reconstruction_error': mse, 'y_test': y_test},)\n",
    "# df_error.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.781300e+04</td>\n",
       "      <td>17813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.816610e+06</td>\n",
       "      <td>0.015943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.891831e+06</td>\n",
       "      <td>0.125260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.446918e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.964878e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.003276e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.158618e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.112221e+08</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error        y_test\n",
       "count          1.781300e+04  17813.000000\n",
       "mean           3.816610e+06      0.015943\n",
       "std            5.891831e+06      0.125260\n",
       "min            4.446918e+03      0.000000\n",
       "25%            1.964878e+06      0.000000\n",
       "50%            3.003276e+06      0.000000\n",
       "75%            5.158618e+06      0.000000\n",
       "max            7.112221e+08      1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3404320.36, 1021893.42, 3587801.42, ..., 3818430.85, 2203437.18, 4414597.13])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (df_error.reconstruction_error > 0.5).tolist()\n",
    "y_pred = [1 if i == True else 0 for i in y_pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Soroosh/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ nan, 0.02]),\n",
       " array([0., 1.]),\n",
       " array([ nan, 0.03]),\n",
       " array([1., 0.]),\n",
       " array([0., 1.]),\n",
       " array([1., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FSCORE = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "PPV, TPR, FSCORE, FNR, FPR, TNR = perf_measure(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Egammas'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-452476a59c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "(1, ) * (2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ven",
   "language": "python",
   "name": "tf_ven"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
