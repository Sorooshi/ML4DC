{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_true, y_pred):\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    FSCORE = np.divide((2*PPV*TPR), (PPV+TPR))\n",
    "    \n",
    "    return PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_particle = 'JetHTs'\n",
    "\n",
    "X_train = np.load(\"matrices/\" + name_of_particle +\"_train.npy\",)\n",
    "y_train = np.load(\"matrices/\" + name_of_particle +\"_y_train.npy\",)\n",
    "X_val = np.load(\"matrices/\" + name_of_particle +\"_val.npy\",)\n",
    "y_val = np.load(\"matrices/\" + name_of_particle +\"_y_val.npy\",)\n",
    "X_test = np.load(\"matrices/\" + name_of_particle +\"_test.npy\",)\n",
    "y_test = np.load(\"matrices/\" + name_of_particle +\"_y_test.npy\",)\n",
    "X_train = X_train[:, :-3]\n",
    "X_val = X_val[:, :-3]\n",
    "X_test = X_test[:, :-3]\n",
    "_, V = X_train.shape\n",
    "K = 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pos = X_train[np.where(y_train==0)]\n",
    "X_train_neg = X_train[np.where(y_train==1)]\n",
    "y_train_pos = y_train[np.where(y_train==0)]\n",
    "y_train_neg = y_train[np.where(y_train==1)]\n",
    "\n",
    "# X_val_pos = X_val[np.where(y_val==0)]\n",
    "# X_val_neg = X_val[np.where(y_val==1)]\n",
    "# y_val_pos = y_val[np.where(y_val==0)]\n",
    "# y_val_neg = y_val[np.where(y_val==1)]\n",
    "\n",
    "# X_test_pos = X_test[np.where(y_test==0)]\n",
    "# X_test_neg = X_test[np.where(y_test==1)]\n",
    "# y_test_pos = y_test[np.where(y_test==0)]\n",
    "# y_test_neg = y_test[np.where(y_test==1)]\n",
    "\n",
    "X_train_pos.shape[0] + X_train_neg.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos = tf.convert_to_tensor(X_train_pos)\n",
    "y_train_pos = tf.convert_to_tensor(y_train_pos)\n",
    "X_train_neg = tf.convert_to_tensor(X_train_neg)\n",
    "y_train_neg = tf.convert_to_tensor(y_train_neg)\n",
    "\n",
    "X_val = tf.convert_to_tensor(X_val)\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# X_val_pos = tf.convert_to_tensor(X_val_pos)\n",
    "# y_val_pos = tf.convert_to_tensor(y_val_pos)\n",
    "# X_val_neg = tf.convert_to_tensor(X_val_neg)\n",
    "# y_val_neg = tf.convert_to_tensor(y_val_neg)\n",
    "\n",
    "\n",
    "# X_test_pos = tf.convert_to_tensor(X_test_pos[1:10, :])\n",
    "# y_test_pos = tf.convert_to_tensor(y_test_pos[1:10])\n",
    "# X_test_neg = tf.convert_to_tensor(X_test_neg[1:10, :])\n",
    "# y_test_neg = tf.convert_to_tensor(y_test_neg[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "\n",
    "# test_ds_pos = tf.data.Dataset.from_tensor_slices((X_test_pos, y_test_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# test_ds_neg = tf.data.Dataset.from_tensor_slices((X_test_neg, y_test_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# test_ds = (test_ds_pos, test_ds_neg)\n",
    "    \n",
    "# train_ds_pos = tf.data.Dataset.from_tensor_slices((X_train_pos, y_train_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# train_ds_neg = tf.data.Dataset.from_tensor_slices((X_train_neg, y_train_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# train_ds = (train_ds_pos, train_ds_neg)\n",
    "\n",
    "# val_ds_pos = tf.data.Dataset.from_tensor_slices((X_val_pos, y_val_pos)).batch(batch_size) #.shuffle(1000)\n",
    "# val_ds_neg = tf.data.Dataset.from_tensor_slices((X_val_neg, y_val_neg)).batch(batch_size) #.shuffle(1000)\n",
    "# val_ds = (val_ds_pos, val_ds_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_pos = 100\n",
    "# N_neg = 10\n",
    "# N_pseudo = 110\n",
    "# V = 20\n",
    "\n",
    "# X_pos = tf.random.normal(shape=[N_pos, V], dtype='float32')\n",
    "# X_neg = tf.random.normal(shape=[N_neg, V], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccModel(Model):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        \n",
    "        super(OccModel, self).__init__(name=name)\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(256, input_shape=(V,),\n",
    "                                            kernel_initializer='uniform', name='dense1')\n",
    "        \n",
    "        self.dense2 = tf.keras.layers.Dense(32,\n",
    "                                           kernel_initializer='uniform',\n",
    "                                            name='dense2')\n",
    "        \n",
    "        self.pred_layer = tf.keras.layers.Dense(1, activation='linear',\n",
    "                                           kernel_initializer='uniform', \n",
    "                                                name='predictions')   \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.pred_layer(x)\n",
    "    \n",
    "def get_model():\n",
    "    return OccModel(name='OneClassClassifier')\n",
    "\n",
    "model_occ = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the custom loss function for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probability(data):\n",
    "    return model_occ(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bf(model_occ, x_pos, x_neg):    \n",
    "    \n",
    "    p_pos = model_occ(x_pos)\n",
    "    p_neg = model_occ(x_neg)\n",
    "    \n",
    "    N_pos, V = x_pos.shape\n",
    "    N_neg, _ = x_neg.shape\n",
    "    N_pseudo = N_pos\n",
    "    \n",
    "    supp_min = tf.reduce_min(x_pos, keepdims=True, axis=0)\n",
    "    supp_max = tf.reduce_max(x_pos, keepdims=True, axis=0)\n",
    "    \n",
    "    x_pseudo = tf.random.uniform(shape=[N_pseudo, V],\n",
    "                                 minval=supp_min-1, maxval=supp_max+1,\n",
    "                                 dtype='float64',)\n",
    "    \n",
    "    p_psedo = model_occ(x_pseudo)\n",
    "    \n",
    "    N_pos = x_pos.shape[0]\n",
    "    N_neg = x_neg.shape[0]\n",
    "    \n",
    "    loss_pos = N_pos / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(-p_pos))\n",
    "    loss_neg = N_neg / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(p_neg))\n",
    "    loss_pseudo = 0.01 * tf.reduce_mean(tf.nn.softplus(p_psedo))\n",
    "    \n",
    "    print(\"loss_bf:\", loss_pos + loss_neg)\n",
    "    \n",
    "    return loss_pos + loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing an optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.BinaryCrossentropy()  (*)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics to measure the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = tf.keras.metrics.Mean(name='train_loss') \n",
    "# train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x_pos, x_neg, labels):\n",
    "    \"\"\"X(x_pos, x_neg, x_pseudo,)\"\"\"\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        \n",
    "        p_pos = model_occ(x_pos)\n",
    "        p_neg = model_occ(x_neg)\n",
    "        N_pos, V = x_pos.shape\n",
    "        N_neg, _ = x_neg.shape\n",
    "        N_pseudo = N_pos\n",
    "    \n",
    "        supp_min = tf.reduce_min(x_pos, keepdims=True, axis=0)\n",
    "        supp_max = tf.reduce_max(x_pos, keepdims=True, axis=0)\n",
    "    \n",
    "        x_pseudo = tf.random.uniform(shape=[N_pseudo, V], \n",
    "                                     minval=supp_min-1, maxval=supp_max+1,\n",
    "                                     dtype='float64',)\n",
    "    \n",
    "        p_psedo = model_occ(x_pseudo)\n",
    "    \n",
    "        N_pos = x_pos.shape[0]\n",
    "        N_neg = x_neg.shape[0]\n",
    "\n",
    "        loss_pos = N_pos / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(-p_pos))\n",
    "        loss_neg = N_neg / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(p_neg))\n",
    "        loss_pseudo = 0.01 * tf.reduce_mean(tf.nn.softplus(p_psedo))\n",
    "\n",
    "        predicions = loss_pos + loss_neg - loss_pseudo\n",
    "        \n",
    "#         predicions = loss_bf(model_occ, x_pos=X_pos, x_neg=X_neg)\n",
    "        print(\"predicions:\", predicions)\n",
    "#         labels = tf.reshape(tf.tile(labels, [1]), [-1, 2])\n",
    "#         loss = loss_object(labels, predicions)  (*)\n",
    "    gradients = tape.gradient(predicions, model_occ.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_occ.trainable_variables))\n",
    "#     train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(X, labels):\n",
    "    predictions = model_occ(X)\n",
    "    labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicions: Tensor(\"add_2:0\", shape=(), dtype=float64)\n",
      "Epoch 1, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 2, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 3, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 4, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 5, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 6, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 7, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 8, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 9, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 10, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 11, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 12, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 13, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 14, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 15, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 16, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 17, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 18, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 19, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 20, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 21, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 22, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 23, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 24, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 25, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 26, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 27, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 28, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 29, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 30, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 31, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 32, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 33, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 34, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 35, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 36, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 37, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 38, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 39, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 40, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 41, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 42, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 43, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 44, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 45, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 46, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 47, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 48, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 49, Validation Loss: 0.137, Validation Accuracy:98.751,\n",
      "Epoch 50, Validation Loss: 0.136, Validation Accuracy:98.751,\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "training_losses_occ, training_accuracies_occ = [], []\n",
    "validations_losses_occ, validations_accuracies_occ = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    results = train_step(x_pos=X_train_pos, x_neg=X_train_neg, labels=y_train)\n",
    "    test_step(X_val, y_val)\n",
    "    \n",
    "#     validations_losses_occ.append(test_loss.result().numpy())\n",
    "#     validations_accuracies_occ.append(test_accuracy.result().numpy())\n",
    "    \n",
    "#     training_losses_occ.append(train_loss.result().numpy())\n",
    "#     training_accuracies_occ.append(train_accuracy.result().numpy())\n",
    "\n",
    "#     validations_losses_occ.append(test_loss.result().numpy())\n",
    "#     validations_accuracies_occ.append(test_accuracy.result().numpy())\n",
    "    \n",
    "    template = 'Epoch {}, Validation Loss: {:.3f}, Validation Accuracy:{:.3f},'\n",
    "    \n",
    "    print (template.format(epoch+1,\n",
    "                           test_loss.result().numpy(),\n",
    "                           test_accuracy.result().numpy()*100),)\n",
    "    \n",
    "model_occ.save_weights(\"NN-ckecks/OCC_Bope\"+ name_of_particle +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-22c113a5e323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_occ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   1496\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model_occ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-22c113a5e323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_occ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   1496\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model_occ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_occ = get_model()\n",
    "new_model_occ.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds:\n",
    "    if cntr == 0:\n",
    "        new_model_occ.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "# new_model_occ.load_weights('NN-ckecks/ThreeLayerNN_model'+ name_of_particle+'.h5')\n",
    "test_predictions = new_model_occ.predict(X_test)\n",
    "probabilities = tf.nn.sigmoid(test_predictions)\n",
    "labels_pred_occ = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "labels_true_occ = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true_occ.append(k)\n",
    "\n",
    "f1_score_occ = precision_recall_fscore_support(labels_true_occ, labels_pred_occ, average='weighted') # Does not take into account labels imbalanced\n",
    "print(\"precision:\", \"%.2f\" %f1_score_occ[0], \"recall:\", \"%.2f\" % f1_score_occ[1], \"fscore:\", \"%.2f\" %f1_score_occ[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1, 0, 1, 1, 0]])\n",
    "y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((1*y_true).numpy(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([0, 1])\n",
    "n_features = 3\n",
    "\n",
    "n_neurons_in_h1 = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(X, dtype=tf.float32, name='X')\n",
    "y = tf.convert_to_tensor(Y, dtype=tf.float32, name='y')\n",
    "\n",
    "W1 = tf.random.normal(shape=[n_features, n_neurons_in_h1], \n",
    "                      mean=0.0, stddev=1/np.sqrt(n_features), name='W1')\n",
    "b1 = tf.random.normal(shape=[n_neurons_in_h1],\n",
    "                      mean=0.0, stddev=1/np.sqrt(n_features), name='b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_ = tf.Variable([[1, -1], [2, -2], [3, -3]], dtype='float32',name='W1')\n",
    "\n",
    "b1_ = tf.Variable([0.4, 0.9], name='b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W1_)\n",
    "print(b1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(x, W1_)+ b1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sigmoid(tf.matmul(x, W1_) + b1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "print(g.watch(x))\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ven",
   "language": "python",
   "name": "tf_ven"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
