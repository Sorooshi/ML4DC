{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(tf.__version__)\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_true, y_pred):\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    FSCORE = np.divide((2*PPV*TPR), (PPV+TPR))\n",
    "    \n",
    "    return PPV, TPR, FSCORE, FNR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_particle = 'Egammas'\n",
    "\n",
    "X_train = np.load(\"matrices/\" + name_of_particle +\"_train.npy\",).astype('float32')\n",
    "y_train = np.load(\"matrices/\" + name_of_particle +\"_y_train.npy\",).astype('float32')\n",
    "X_val = np.load(\"matrices/\" + name_of_particle +\"_val.npy\",).astype('float32')\n",
    "y_val = np.load(\"matrices/\" + name_of_particle +\"_y_val.npy\",).astype('float32')\n",
    "X_test = np.load(\"matrices/\" + name_of_particle +\"_test.npy\",).astype('float32')\n",
    "y_test = np.load(\"matrices/\" + name_of_particle +\"_y_test.npy\",).astype('float32')\n",
    "X_train = X_train[:, :-3]\n",
    "X_val = X_val[:, :-3]\n",
    "X_test = X_test[:, :-3]\n",
    "N, V = X_train.shape\n",
    "K = 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_min = np.min(X_train, keepdims=True, axis=0)\n",
    "supp_max = np.max(X_train, keepdims=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)  #.shuffle(1000)  \n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size) #.shuffle(1000)  #shuffle(1000)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)  #.shuffle(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccModel(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(OccModel, self).__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(256, input_shape=(V,),\n",
    "                                            kernel_initializer='uniform', name='dense1')\n",
    "        \n",
    "        self.dense2 = tf.keras.layers.Dense(32,\n",
    "                                           kernel_initializer='uniform',\n",
    "                                            name='dense2')\n",
    "        \n",
    "        self.pred_layer = tf.keras.layers.Dense(1, activation='linear',\n",
    "                                           kernel_initializer='uniform', \n",
    "                                                name='predictions')   \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.pred_layer(x)\n",
    "    \n",
    "\n",
    "model_occ = OccModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the custom loss function for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*)\n",
    "def loss_bf(model, X_tr, y_tr, x_pos, x_neg): \n",
    "    \n",
    "#     x_pos = X_tr[np.where(y_tr==0)]\n",
    "#     x_neg = X_tr[np.where(y_tr==1)]\n",
    "    \n",
    "    p_pos = model(x_pos)\n",
    "    p_neg = model(x_neg)\n",
    "    \n",
    "    N_pos, V = x_pos.shape\n",
    "    N_neg, _ = x_neg.shape\n",
    "    \n",
    "#     supp_min = tf.reduce_min(x_neg, keepdims=True, axis=0)\n",
    "#     supp_max = tf.reduce_max(x_neg, keepdims=True, axis=0)\n",
    "    \n",
    "    x_pseudo = tf.random.uniform(shape=[N_pos, V],\n",
    "                                 minval=supp_min-3, maxval=supp_max+3,\n",
    "                                 dtype='float32',)\n",
    "    \n",
    "    p_psedo = model_occ(x_pseudo)\n",
    "    \n",
    "    loss_pos = N_pos / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(-p_pos))\n",
    "    loss_neg = N_neg / (N_pos + N_neg) * tf.reduce_mean(tf.nn.softplus(p_neg)) \n",
    "    loss_pseudo = 0.001 * tf.reduce_mean(tf.nn.softplus(p_psedo))\n",
    "    \n",
    "    preds = tf.nn.sigmoid(model_occ(X_tr))\n",
    "    \n",
    "    return loss_pos + loss_neg + loss_pseudo, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_bf(model, X_tr, y_tr,):  \n",
    "    \n",
    "#     x_pos = X_train[np.where(y_train==0)]\n",
    "#     x_neg = X_train[np.where(y_train==1)]\n",
    "    \n",
    "#     p_pos = model(x_pos)\n",
    "#     p_neg = model(x_neg)\n",
    "    \n",
    "#     N_pos, V = x_pos.shape\n",
    "#     N_neg, _ = x_neg.shape\n",
    "    \n",
    "#     supp_min = tf.reduce_min(x_neg, keepdims=True, axis=0)\n",
    "#     supp_max = tf.reduce_max(x_neg, keepdims=True, axis=0)\n",
    "    \n",
    "#     x_pseudo = tf.random.uniform(shape=[N_pos, V],\n",
    "#                                  minval=supp_min-3, maxval=supp_max+3,\n",
    "#                                  dtype='float32',)\n",
    "    \n",
    "#     p_psedo = model_occ(x_pseudo)\n",
    "    \n",
    "#     loss_pos = N_pos / (N_pos + N_neg) * (tf.nn.softplus(-p_pos))\n",
    "#     loss_neg = N_neg / (N_pos + N_neg) * (tf.nn.softplus(p_neg))\n",
    "#     loss_pseudo = 0.01 * (tf.nn.softplus(p_psedo))\n",
    "    \n",
    "#     print(\"loss_bf pos:\", loss_pos,)\n",
    "#     print(\" \")\n",
    "#     print(\"loss_bf neg:\", loss_neg,)\n",
    "#     print(\" \")\n",
    "#     print(\"loss_bf pseudo:\", loss_pseudo)\n",
    "#     print(\" \")\n",
    "    \n",
    "#     preds = tf.nn.sigmoid(model_occ(X_tr))\n",
    "#     return loss_pos, loss_neg, loss_pseudo, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing an optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy() \n",
    "optimizer = tf.keras.optimizers.Adam(1e-6) # 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics to measure the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss') \n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*)\n",
    "@tf.function\n",
    "def train_step(X_tr, y_tr, x_pos, x_neg): \n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        loss, preds = loss_bf(model=model_occ, X_tr=X_tr, y_tr=y_tr, x_pos=x_pos, x_neg=x_neg)  # x_pos=x_pos, x_neg=x_neg, x_pseudo=x_pseudo\n",
    "        # y_tr = tf.reshape(tf.tile(y_tr, [2]), [-1, 2])\n",
    "        loss_ = loss_object(y_tr, preds)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model_occ.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_occ.trainable_variables))\n",
    "    \n",
    "    train_loss(loss_)\n",
    "    train_accuracy(y_tr, preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(X_tr, y_tr):  # x_pos, x_neg, x_pseudo\n",
    "#     with tf.GradientTape() as tape_pos, tf.GradientTape() as tape_neg, tf.GradientTape() as tape_psuedo:  # Record operations for automatic differentiation\n",
    "#         loss_pos, loss_neg, loss_pseudo, preds = loss_bf(model=model_occ, X_tr=X_tr, y_tr=y_tr )  \n",
    "#         labels = y_tr # tf.reshape(tf.tile(y_tr, [2]), [-1, 2])\n",
    "#         loss_ = loss_object(labels, preds)\n",
    "        \n",
    "#     gradients_pos = tape_pos.gradient(loss_pos, model_occ.trainable_variables)\n",
    "#     gradients_neg = tape_neg.gradient(loss_neg, model_occ.trainable_variables)\n",
    "#     gradients_psuedo = tape_psuedo.gradient(loss_pseudo, model_occ.trainable_variables)\n",
    "# #     gradients = gradients_pos + gradients_neg + gradients_psuedo\n",
    "# #     print(\"gradients:\", gradients)\n",
    "# #     optimizer.apply_gradients(zip(gradients, model_occ.trainable_variables))\n",
    "\n",
    "#     print(\"gradients_pos:\", gradients_pos)\n",
    "#     print(\" \")\n",
    "#     print(\"gradients_neg:\", gradients_neg)\n",
    "#     print(\" \")\n",
    "#     print(\"gradients_psuedo:\", gradients_psuedo)\n",
    "#     print(\" \")\n",
    "\n",
    "\n",
    "#     optimizer.apply_gradients(zip(gradients_pos, model_occ.trainable_variables))\n",
    "#     optimizer.apply_gradients(zip(gradients_neg, model_occ.trainable_variables))\n",
    "#     optimizer.apply_gradients(zip(gradients_psuedo, model_occ.trainable_variables))\n",
    "    \n",
    "    \n",
    "#     train_loss(loss_)\n",
    "#     train_accuracy(labels, preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(X, labels):\n",
    "    predictions = model_occ(X)\n",
    "#     labels = tf.reshape(tf.tile(labels, [2]), [-1, 2])\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "training_losses_occ, training_accuracies_occ = [], []\n",
    "validations_losses_occ, validations_accuracies_occ = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for X_tr, y_tr in train_ds:\n",
    "        x_pos = tf.convert_to_tensor(X_tr.numpy()[np.where(y_tr.numpy()==0)])\n",
    "        x_neg = tf.convert_to_tensor(X_tr.numpy()[np.where(y_tr.numpy()==1)])\n",
    "        results = train_step(X_tr=X_tr, y_tr=y_tr, x_pos=x_pos, x_neg=x_neg)\n",
    "        \n",
    "        training_losses_occ.append(train_loss.result().numpy())\n",
    "        training_accuracies_occ.append(train_accuracy.result().numpy())\n",
    "        \n",
    "    for X_val, y_val in val_ds:\n",
    "        test_step(X_val, y_val)\n",
    "        \n",
    "        validations_losses_occ.append(test_loss.result().numpy())\n",
    "        validations_accuracies_occ.append(test_accuracy.result().numpy())\n",
    "    \n",
    "    template = 'Epoch {}, Train Loss: {:.3f}, Train Accuracy:{:.3f}, \\\n",
    "    Validation Loss: {:.3f}, Validation Accuracy:{:.3f},'\n",
    "    \n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result().numpy(),\n",
    "                         train_accuracy.result().numpy()*100,\n",
    "                           \n",
    "                         test_loss.result().numpy(),\n",
    "                         test_accuracy.result().numpy()*100),)\n",
    "    \n",
    "     # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    \n",
    "model_occ.save_weights(\"NN-ckecks/OCC_Bope\"+ name_of_particle +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"occ_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               multiple                  54272     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               multiple                  8224      \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          multiple                  33        \n",
      "=================================================================\n",
      "Total params: 62,529\n",
      "Trainable params: 62,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_occ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.99,  nan]),\n",
       " array([1., 0.]),\n",
       " array([ 1., nan]),\n",
       " array([0., 1.]),\n",
       " array([0., 1.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_occ = OccModel()\n",
    "new_model_occ.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6))\n",
    "\n",
    "# Since In this implementation instead of weight we are dealing \n",
    "# with codes and classes therefore the traditional serialization and\n",
    "# deserialization is not possible. So we have to first initialze\n",
    "# the model (which is code) and then load the weights \n",
    "# Ref: https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=OOSGiSkHTERy\n",
    "\n",
    "cntr = 0\n",
    "for i, j in train_ds:\n",
    "    if cntr == 0:\n",
    "        new_model_occ.train_on_batch(i[:1], j[:1])\n",
    "    cntr += 1 \n",
    "\n",
    "# new_model_occ.load_weights('NN-ckecks/ThreeLayerNN_model'+ name_of_particle+'.h5')\n",
    "test_predictions = new_model_occ.predict(X_test)\n",
    "probabilities = tf.nn.sigmoid(test_predictions)\n",
    "labels_pred_occ = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "labels_true_occ = []\n",
    "for i, j in test_ds:\n",
    "    for k in j.numpy():\n",
    "        labels_true_occ.append(k)\n",
    "\n",
    "PPV3, TPR3, FSCORE3, FNR3, FPR3, TNR3 = perf_measure(y_true=labels_true_occ, y_pred=labels_pred_occ)\n",
    "\n",
    "\n",
    "PPV3, TPR3, FSCORE3, FNR3, TNR3, # FPR3, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([0., 1.]), array([1, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_curve(labels_true_occ, labels_pred_occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= np.array([1, 0, 3, 0])\n",
    "D = np.array([1, 2, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0., inf, nan])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sanity_check(object):\n",
    "#     def __init__(self):\n",
    "#         super(sanity_check, self).__init__\n",
    "#         self.W = tf.Variable(tf.random.normal(shape=[2, 3]), name='W')\n",
    "#         self.b = tf.Variable(tf.random.normal(shape=[2,]), name='b')\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return self.W*x\n",
    "        \n",
    "# model = sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = tf.Variable(tf.random.normal(shape=[2, 3]), name='W')\n",
    "# W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= tf.Variable(np.array([[1, 2, 3], [10, 12, 13],]).astype(\"float32\"))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sanity_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
